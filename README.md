# To_Do_List

# paper 多专家 2024年08月07日

Efficient Deweahter Mixture-of-Experts with Uncertainty-Aware Feature-Wise Linear Modulation

具有不确定性感知功能的高效Deweahter混合专家线性调制

Demystifying Softmax Gating Function in Gaussian Mixture of Experts

揭秘专家高斯混合中的Softmax门控函数

MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts

模式：专家之间相互蒸馏的混合专家模型

Multi-Task Dense Prediction via Mixture of Low-Rank Experts

基于低阶专家混合的多任务密集预测

MoME: Mixture-of-Masked-Experts for Efficient Multi-Task Recommendation ☆

MoME：高效多任务推荐的Mixture-of-Masked-Experts☆




# Paper 2024年08月01日

Quantifying Task Priority for Multi-Task Optimization 

Cloud-Device Collaborative Learning for Multimodal Large Language Models

Asymmetric Masked Distillation for Pre-Training Small Foundation Models

Bootstrapping SparseFormers from Vision Foundation Models

PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution

PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor

Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts

OmniGlue: Generalizable Feature Matching with Foundation Model Guidance

OMG-Seg: Is One Model Good Enough For All Segmentation?

Multi-Task Dense Prediction via Mixture of Low-Rank Experts

Jack of All Tasks Master of Many: Designing General-Purpose Coarse-to-Fine Vision-Language Model

？ Reconstruction-free Cascaded Adaptive Compressive Sensing 

Retraining-Free Model Quantization via One-Shot Weight-Coupling Learning

Rethinking Multi-view Representation Learning via Distilled Disentangling

Retraining-Free Model Quantization via One-Shot Weight-Coupling Learning

Training-Free Pretrained Model Merging ☆ 

UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory ☆ 

UniPTS: A Unified Framework for Proficient Post-Training Sparsity

Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models

OTOV2: Automatic, Generic, User-Friendly

Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch

Network Expansion for Practical Training Acceleration

Get More at Once: Alternating Sparse Training with Gradient Correction

PTQ4SAM: Post-Training Quantization for Segment Anything

A Fast Post-Training Pruning Framework for Transformers

Retraining-free Model Quantization via One-ShotWeight-Coupling Learning

TinySAM: Pushing the Envelope for Efficient Segment Anything Model

Learning Pruning-Friendly Networks Via Frank-Wolfe: One-Shot, Any-Sparsity, And No Retraining

Train Once, and Decode As You Like

Get More at Once: Alternating Sparse Training with Gradient Correction

Structurally Prune Anything: Any Architecture, Any Framework, Any Time

E2E-AT: A Unified Framework for Tackling Uncertainty in Task-Aware End-to-End Learning

DeepSpeed Data Effciency: Improving Deep Learning Model Quality and Training Effciency via Effcient Data Sampling and Routing

MIND: Multi-Task Incremental Network Distillation

Data Shunt: Collaboration of Small and Large Models for Lower Costs and Better Performance

[no code] EPSD:Early Pruning with Self-Distillation for Efficient Model Compression

[no code] One Step Learning, One Step Review

Partial Label Learning with a Partner

Towards Real-World Test-Time Adaptation: Tri-net Self-Training with Balanced Normalization

Amalgamating Multi-Task Models with Heterogeneous Architectures ☆ 尝试过，没成功

# 微调

[no code] Measuring Task Similarity and Its Implication in Fine-Tuning Graph Neural Network



# 教师-多任务-模型

Class Incremental Learning with Multi-Teacher Distillation

Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners ☆

AAAI24' FedCD: Federated Semi-Supervised Learning with Class Awareness Balance via Dual Teachers

AAAI24' Let All Be Whitened: Multi-Teacher Distillation for Efficient Visual Retrieval

SAM-PARSER: Fine-Tuning SAM Efficiently by Parameter Space Reconstruction

Let All Be Whitened: Multi-Teacher Distillation for Efficient Visual Retrieval

Collaborative Consortium of Foundation Models for Open-World Few-Shot Learning

Say Anything with Any Style

How to Trade Off the Quantity and Capacity of Teacher Ensemble: Learning Categorical Distribution to Stochastically Employ A 

Teacher for Distillation 

[no code] Efficient Deweahter Mixture-of-Experts with Uncertainty-Aware Feature-Wise Linear Modulation

[no code] Learning Multi-Task Sparse Representation Based on Fisher Information

[no code]  Teacher as a Lenient Expert: Teacher-Agnostic Data-Free Knowledge Distillation

# 架构搜索论文

Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach

Building Optimal Neural Architectures using Interpretable Knowledge

Towards Accurate and Robust Architectures via Neural Architecture Search



# 来自aaai2024 有点关联的论文

E2E-AT: A Unified Framework for Tackling Uncertainty in Task-Aware End-to-End Learning

剪枝论文列表：https://github.com/ghimiredhikura/Awasome-Pruning

auc问题：AUC Optimization from Multiple Unlabeled Datasets


